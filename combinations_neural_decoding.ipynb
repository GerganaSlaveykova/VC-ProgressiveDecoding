{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Decoding - Progressive additive approach\n",
        "##Written by Gergana Slaveykova s1070004\n",
        "##Radboud University- B3 Thesis project"
      ],
      "metadata": {
        "id": "m68AfAs2O4Bp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbAWd6qWCF1z"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W58YcSsKCLnv"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive to acess data and storing purposes\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgMRNYz7B8a5"
      },
      "outputs": [],
      "source": [
        "#Installs\n",
        "! pip install mxnet\n",
        "! pip install nilearn\n",
        "! pip install nibabel\n",
        "!pip install git+https://github.com/nipy/nipy.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "senBToFlCJC1"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from nipy.modalities.fmri.experimental_paradigm import load_paradigm_from_csv_file\n",
        "from nipy.modalities.fmri.design_matrix import make_dmtx\n",
        "from nipy.labs.viz import plot_map, cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from types import ModuleType\n",
        "from typing import Tuple, Union\n",
        "\n",
        "from nilearn import masking, plotting\n",
        "from PIL import Image\n",
        "from scipy.stats import t, zscore\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import h5py\n",
        "import PIL.Image\n",
        "from scipy import sparse, stats\n",
        "from numpy.linalg import svd\n",
        "import time\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The combinations code is not optimal. What I did is to enter the combination that I wanted manually. You can in practice do this with function. At the moment while doing the project I found it better for me to have more control on what is going in each combination."
      ],
      "metadata": {
        "id": "e00aNLnLMWzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main logic"
      ],
      "metadata": {
        "id": "Rc9XBYBwMUfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to experiment with z-scoring and clipping\n",
        "def clip_and_z_score(train,test):\n",
        "  x_tr = np.clip(train, -3, 3)\n",
        "  x_te_pt = np.clip(test, -3, 3)\n",
        "\n",
        "  # Z-score normalization\n",
        "  # Calculate mean and standard deviation on training data\n",
        "  norm_mean_x = np.mean(x_tr, axis=0)\n",
        "  norm_std_x = np.std(x_tr, axis=0, ddof=1)\n",
        "  # Avoid division by zero\n",
        "  norm_std_x[norm_std_x == 0] = 1\n",
        "\n",
        "  # Normalization\n",
        "  x_tr_normalized = (x_tr - norm_mean_x) / norm_std_x\n",
        "  x_te_pt_normalized = (x_te_pt - norm_mean_x) / norm_std_x\n",
        "\n",
        "  # Z-scoring the whole dataset\n",
        "  #x_whole_normalized = (x - np.mean(x, axis=0)) / np.std(x, axis=0, ddof=1)\n",
        "\n",
        "  return x_tr_normalized,x_te_pt_normalized"
      ],
      "metadata": {
        "id": "tFtdl_oK6HRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def z_score(x_tr,x_te_pt):\n",
        "  # Z-score normalization\n",
        "  # Calculate mean and standard deviation on training data\n",
        "  norm_mean_x = np.mean(x_tr, axis=0)\n",
        "  norm_std_x = np.std(x_tr, axis=0, ddof=1)\n",
        "  # Avoid division by zero\n",
        "  norm_std_x[norm_std_x == 0] = 1\n",
        "\n",
        "  # Normalization\n",
        "  x_tr_normalized = (x_tr - norm_mean_x) / norm_std_x\n",
        "  x_te_pt_normalized = (x_te_pt - norm_mean_x) / norm_std_x\n",
        "\n",
        "  # Z-scoring the whole dataset\n",
        "  #x_whole_normalized = (x - np.mean(x, axis=0)) / np.std(x, axis=0, ddof=1)\n",
        "\n",
        "  return x_tr_normalized,x_te_pt_normalized"
      ],
      "metadata": {
        "id": "zV8lbA2LLM7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsjtFaIJCab5"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzWK8d8RCZnI"
      },
      "outputs": [],
      "source": [
        "#Load latents of the images\n",
        "w_te= np.load(\"/content/drive/MyDrive/GOD-NEW-3p/latents/t_te_best.npy\")\n",
        "w_tr= np.load(\"/content/drive/MyDrive/GOD-NEW-3p/latents/t_tr_best.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load indices of the voxel selection\n",
        "file_path = \"/content/drive/MyDrive/GOD-NEW-3p/Neural-DecodingPartProgresive/indeces_filtered_clipped3.dat\"\n",
        "importantind = np.load(file_path,allow_pickle=True)\n",
        "for i in importantind:\n",
        "  print(len(i))\n"
      ],
      "metadata": {
        "id": "QzDTkIlM26_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0UJSyOsIoir"
      },
      "outputs": [],
      "source": [
        "#Areas of interest.You can comment out given one depenfing on what combination you want\n",
        "rois = {\n",
        "        'V1' : 'ROI_V1 = 1',\n",
        "        'V2' : 'ROI_V2 = 1',\n",
        "        'V3' : 'ROI_V3 = 1',\n",
        "        # 'V4' : 'ROI_V4 = 1',\n",
        "        # 'LOC' : 'ROI_LOC = 1',\n",
        "        # 'FFA' : 'ROI_FFA = 1',\n",
        "        # 'PPA' : 'ROI_PPA = 1'\n",
        "        }\n",
        "\n",
        "rois.keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load brain data\n",
        "file_path = \"/content/drive/MyDrive/GOD-NEW-3p/Neural-DecodingPartProgresive/indeces_filtered_clipped3.dat\"\n",
        "importantind = np.load(file_path,allow_pickle=True)\n",
        "\n",
        "x_tr = []\n",
        "x_te = []\n",
        "#iterate over all regions\n",
        "for roi_index, roi in enumerate(rois):\n",
        "    _roi = np.load(f\"/content/drive/MyDrive/GOD-NEW-3p/my_experiment_{roi}_hyperaligned.npy\")  # load in roi, all voxels, you want to select a mask\n",
        "    tr=_roi[:1200,importantind[roi_index]] #filter the voxels\n",
        "    te=_roi[1200:1250,importantind[roi_index]]\n",
        "    #if you want you can z-score\n",
        "    #tr,te= z_score(tr,te)\n",
        "    #tr,te= clip_and_z_score(tr,te)\n",
        "    x_tr.append(tr)\n",
        "    x_te.append(te)\n",
        "\n",
        "print(len(x_te))"
      ],
      "metadata": {
        "id": "MpRITsMIk5lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine the regions together so they can be given to the linear model easly\n",
        "concat_te = np.concatenate(x_te, axis=1)\n",
        "print(concat_te.shape)\n",
        "\n",
        "concat_tr = np.concatenate(x_tr, axis=1)\n",
        "print(concat_tr.shape)"
      ],
      "metadata": {
        "id": "zEUVNPtKDT6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DASJVt0gE2kb",
        "outputId": "44b51757-118c-4af9-c9b3-9c36bee1bde6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 512)\n"
          ]
        }
      ],
      "source": [
        "#train linear model on the combined filtered braind data\n",
        "modelWhole = LinearRegression().fit(concat_tr, w_tr)\n",
        "#predicted latents\n",
        "y_teWhole = modelWhole.predict(concat_te)\n",
        "\n",
        "print(y_teWhole.shape)\n",
        "\n",
        "#choose directory\n",
        "file_path=\"/content/drive/MyDrive/filtered_my_method/\"\n",
        "\n",
        "#pick the appropriate name for the file and store it\n",
        "with open(f'{file_path}v3v2v1_y_te.npy', 'wb') as fp:\n",
        "            pickle.dump(y_teWhole, fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
