{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Voxel Selection\n",
        "\n",
        "##Written by Gergana Slaveykova s1070004\n",
        "##Radboud University- B3 Thesis project"
      ],
      "metadata": {
        "id": "bmWubIaETc_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "oOYlkEuNwTtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZzCaUOiwQZ9"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive to acess data and storing purposes\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs\n",
        "!pip install nibabel\n",
        "!pip install nilearn\n",
        "!pip install git+https://github.com/nipy/nipy.git\n",
        "\n",
        "#Imports\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from nipy.modalities.fmri.experimental_paradigm import load_paradigm_from_csv_file\n",
        "from nipy.modalities.fmri.design_matrix import make_dmtx\n",
        "from nipy.labs.viz import plot_map, cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from __future__ import annotations\n",
        "import os\n",
        "from types import ModuleType\n",
        "from typing import Tuple, Union\n",
        "from nilearn import masking, plotting\n",
        "from PIL import Image\n",
        "from scipy.stats import t, zscore\n",
        "from sklearn.linear_model import RidgeCV\n",
        "import h5py\n",
        "import PIL.Image\n",
        "from scipy import sparse, stats\n",
        "from scipy.stats import t, zscore\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from numpy.linalg import svd\n",
        "import time\n",
        "from scipy import signal\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "ZA9hI8sJwiwA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features loading"
      ],
      "metadata": {
        "id": "fcAxEJ0rwpjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of layers\n",
        "num_layers = 5\n",
        "\n",
        "#Set the feature paths\n",
        "train_file_paths = [f'/content/drive/My Drive/newDataExperimentationGOD/objects_features_vgg_tr_{i}.npy' for i in range(0, num_layers + 1)]\n",
        "test_file_paths = [f'/content/drive/My Drive/newDataExperimentationGOD/objects_features_vgg_te_{i}.npy' for i in range(0, num_layers + 1)]\n",
        "\n",
        "#feature array initialization\n",
        "train_features = []\n",
        "test_features = []\n",
        "\n",
        "# Load and process train features\n",
        "for i in range(num_layers):\n",
        "    print(np.load(train_file_paths[i]).shape)\n",
        "    train_features.append(np.load(train_file_paths[i]).reshape(1200 if i < 5 else 1200, -1).squeeze())\n",
        "    print(f\"Layer {i + 1} - Train features: {train_features[i].shape}\")\n",
        "\n",
        "# Print separator\n",
        "print(20 * \"=\")\n",
        "\n",
        "# Load and process test features\n",
        "for i in range(num_layers):\n",
        "    print(np.load(test_file_paths[i]).shape)\n",
        "    print(np.load(test_file_paths[i]).reshape(50 if i < 5 else 50, -1).shape)\n",
        "    print(np.load(test_file_paths[i]).reshape(50 if i < 5 else 50, -1).squeeze().shape)\n",
        "    test_features.append(np.load(test_file_paths[i]).reshape(50 if i < 5 else 50, -1).squeeze())\n",
        "    print(f\"Layer {i + 1} - Test features: {test_features[i].shape}\")"
      ],
      "metadata": {
        "id": "Mjb3DrRUwo03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear model"
      ],
      "metadata": {
        "id": "MzRCXXNiwwKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Arhitecture of Kernel CV Rigde regression\n",
        "class KernelRidgeCV:\n",
        "    def __init__(self, kernel, target, n_lambdas):\n",
        "        self.kernel = kernel  # Precomputed kernel matrix\n",
        "        self.target = target  # Target values for the regression\n",
        "        self.n_lambdas = n_lambdas  # Number of lambda (regularization) values to consider\n",
        "        self._lambdas = None  # Placeholder for lambda values\n",
        "        self._df = None  # Placeholder for degrees of freedom\n",
        "\n",
        "    @property\n",
        "    def lambdas(self):\n",
        "        if self._lambdas is not None:\n",
        "            return self._lambdas\n",
        "\n",
        "        # Singular Value Decomposition of the kernel matrix\n",
        "        s = svd(self.kernel)[1]\n",
        "        s = s[s > 0]\n",
        "\n",
        "        self._lambdas = np.full((self.n_lambdas), np.nan)\n",
        "        length = s.shape[0]\n",
        "        self._df = np.linspace(length, 1, self.n_lambdas)\n",
        "        mean = np.mean(1/s)\n",
        "\n",
        "        # Function to find the difference between desired and actual degrees of freedom\n",
        "        f = lambda df, lamb: df - np.sum(s / (s + lamb))\n",
        "        f_prime = lambda lamb: np.sum(s / (s + lamb)**2)\n",
        "\n",
        "        # get all the lambdas\n",
        "        for i in range(1, self.n_lambdas):\n",
        "            if i == 1:\n",
        "                self._lambdas[i] = 0\n",
        "            else:\n",
        "                self._lambdas[i] = self._lambdas[i-1]\n",
        "            self._lambdas[i] = max(self._lambdas[i], (length / self._df[i] - 1) / mean)\n",
        "            temp = f(self._df[i], self._lambdas[i])\n",
        "            # Use Newton-Raphson method to refine lambda values\n",
        "            while abs(temp) > 1e-10:\n",
        "                self._lambdas[i] = max(0, self._lambdas[i] - temp / f_prime(self._lambdas[i]))\n",
        "                temp = f(self._df[i], self._lambdas[i])\n",
        "        return self._lambdas[1:]\n",
        "\n",
        "\n",
        "    #train loop\n",
        "    def train(self, X):\n",
        "        best_model, best_error = None, np.inf\n",
        "\n",
        "        # Cross-validation over all lambda values\n",
        "        for lambda_, df_ in zip(self.lambdas, self._df):\n",
        "            # Initialize Kernel Ridge Regression model with the current lambda\n",
        "            kernel_ridge = KernelRidge(alpha=lambda_)\n",
        "            kernel_ridge.fit(X, self.target)\n",
        "            y = kernel_ridge.predict(X)\n",
        "\n",
        "            # Compute the error, avoiding division by zero\n",
        "            print((1 - df_ / self.kernel.shape[0]))\n",
        "            if  (1 - df_ / self.kernel.shape[0]) != 0:\n",
        "              error = np.sum(((self.target - y) / (1 - df_ / self.kernel.shape[0])) ** 2)\n",
        "            else:\n",
        "            # Set the error to negative infinity to penalize this case\n",
        "              error = np.\n",
        "            #this version was having division by 0 which was giving a warning for our case\n",
        "            #error = np.sum(((self.target - y) / (1 - df_ / self.kernel.shape[0])) ** 2)\n",
        "            print(f\"curr error: {error}\")\n",
        "            if error < best_error:\n",
        "                best_error = error\n",
        "                best_model = kernel_ridge\n",
        "        print(\"Best error:\", best_error, \"Alpha: \", best_model.alpha)\n",
        "        return best_model"
      ],
      "metadata": {
        "id": "6_SrqcA4wvee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_correlations_KERNEL(features_train_layer,features_test_layer,x_tr,x_te):\n",
        "    \"\"\"\n",
        "    Computes the Pearson correlation coefficients between predicted and actual test target values using\n",
        "    Kernel Ridge Regression with cross-validated lambda (regularization) values.\n",
        "\n",
        "    Parameters:\n",
        "    - features_train_layer (np.array): Training set feature representations. These are the features extracted from a particular layer of a model.\n",
        "    - features_test_layer (np.array): Test set feature representations. These are the features extracted from a particular layer of a model.\n",
        "    - x_tr (np.array): Target values corresponding to the training set.\n",
        "    - x_te (np.array): Target values corresponding to the test set.\n",
        "\n",
        "    Returns:\n",
        "    - correlation_coefficients (list): List of Pearson correlation coefficients between predicted and actual test target values.\n",
        "    - x_hat (np.array): Predicted target values for the test set.\n",
        "    \"\"\"\n",
        "    n_te = np.array(x_te).shape[0]# nr examples, test set\n",
        "    n_tr = np.array(x_tr).shape[0] # nr examples. training set\n",
        "\n",
        "\n",
        "    # number of lambda values to test w/ grid search\n",
        "    n = 10\n",
        "    print(f\"Shaape of features test {np.array(features_test_layer).shape}\")\n",
        "    print(f\"Shaape of features test {np.array(features_train_layer).shape}\")\n",
        "    # Reshape feature matrices\n",
        "    f_te = features_test_layer.reshape(n_te, -1)\n",
        "    f_tr = features_train_layer.reshape(n_tr, -1)\n",
        "\n",
        "    # k-Ridge @ multiplication brain2gan methods neuroencoding\n",
        "    # Compute the kernel matrix\n",
        "    kernel = f_tr @ f_tr.T\n",
        "    kernel = kernel.astype(float)\n",
        "    # Initialize Kernel Ridge Regression with cross-validation\n",
        "    ridge_cv = KernelRidgeCV(kernel, x_tr, n)\n",
        "    model = ridge_cv.train(f_tr)\n",
        "\n",
        "    # Predict target values for the test set\n",
        "    x_hat = model.predict(f_te)\n",
        "    print(\"Alpha:\", model.alpha)\n",
        "\n",
        "    y_test = x_te\n",
        "    # Calculate the Pearson correlation coefficient for the entire response vectors\n",
        "    correlation_coefficients =pearson_correlation_coefficient(x_hat,y_test,0)\n",
        "    print(len(correlation_coefficients[0]))\n",
        "    return correlation_coefficients,x_hat"
      ],
      "metadata": {
        "id": "biqOdnxDwzkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pearson_correlation_coefficient(x: np.ndarray, y: np.ndarray, axis: int) -> np.ndarray:\n",
        "  \"\"\"\n",
        "    Calculates the Pearson correlation coefficient and the corresponding p-values between two arrays along the specified axis.\n",
        "\n",
        "    Parameters:\n",
        "    - x (np.ndarray): predicted data\n",
        "    - y (np.ndarray): original data\n",
        "\n",
        "    Returns:\n",
        "    - r (np.ndarray): Pearson correlation coefficients.\n",
        "    - p (np.ndarray): p-values for testing non-correlation.\n",
        "    \"\"\"\n",
        "    # Standardize x and y using z-score normalization\n",
        "    r = (np.nan_to_num(zscore(x)) * np.nan_to_num(zscore(y))).mean(axis)\n",
        "    # Calculate p-values for the correlation coefficients\n",
        "    p = 2 * t.sf(np.abs(r / np.sqrt((1 - r ** 2) / (x.shape[0] - 2))), x.shape[0] - 2)\n",
        "    return r, p"
      ],
      "metadata": {
        "id": "dDAvcT3zxE6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_and_z_score(train,test):\n",
        "    \"\"\"\n",
        "    Applies clipping and z-score normalization to training and test data.\n",
        "\n",
        "    Parameters:\n",
        "    - train (np.ndarray): Training data.\n",
        "    - test (np.ndarray): Test data.\n",
        "\n",
        "    Returns:\n",
        "    - x_tr_normalized (np.ndarray): Clipped and z-score normalized training data.\n",
        "    - x_te_pt_normalized (np.ndarray): Clipped and z-score normalized test data.\n",
        "    \"\"\"\n",
        "    # Clip the values of the training and test data to be within the range [-3, 3]\n",
        "    x_tr = np.clip(train, -3, 3)\n",
        "    x_te_pt = np.clip(test, -3, 3)\n",
        "\n",
        "    # Z-score normalization\n",
        "    # Calculate mean and standard deviation on training data\n",
        "    norm_mean_x = np.mean(x_tr, axis=0)\n",
        "    norm_std_x = np.std(x_tr, axis=0, ddof=1)\n",
        "    # Avoid division by zero\n",
        "    norm_std_x[norm_std_x == 0] = 1\n",
        "\n",
        "    # Normalization\n",
        "    x_tr_normalized = (x_tr - norm_mean_x) / norm_std_x\n",
        "    x_te_pt_normalized = (x_te_pt - norm_mean_x) / norm_std_x\n",
        "\n",
        "    # Z-scoring the whole dataset if needed\n",
        "    #x_whole_normalized = (x - np.mean(x, axis=0)) / np.std(x, axis=0, ddof=1)\n",
        "\n",
        "    return x_tr_normalized,x_te_pt_normalized"
      ],
      "metadata": {
        "id": "jywoSrfDxGp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main logic"
      ],
      "metadata": {
        "id": "Crm5_224wYiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#regions of interest\n",
        "regions=['V1','V2','V3','V4', 'LOC','FFA','PPA']\n",
        "\n",
        "layers_final=[]\n",
        "layers_final_full=[]\n",
        "indices_all_areas=[]\n",
        "important_indeces=[]\n",
        "number_of_model=0\n",
        "important_correlations_total=[]\n",
        "\n",
        "for area in regions:\n",
        "  # Load hyperaligned data for the given brain area\n",
        "  x = np.load(f\"/content/drive/MyDrive/GOD-NEW-3p/my_experiment_{area}_hyperaligned.npy\")\n",
        "  print(x.shape)\n",
        "  # Split the data to get training set\n",
        "  x_tr = x[:1200]\n",
        "  correlation_coefficients_list = []\n",
        "  y_pred_list = []\n",
        "  # Placeholder for predictions\n",
        "  y_heat = np.zeros_like(x_tr)\n",
        "  # Placeholder for cumulative correlations\n",
        "  corr_total= np.zeros_like(x_tr)\n",
        "\n",
        "\n",
        "  # Iterate through the layers\n",
        "  for i in range(len(train_features)):\n",
        "      # Initialize K-Fold cross-validation with 5 splits\n",
        "      kf = KFold(n_splits=5)\n",
        "      # Perform K-Fold cross-validation\n",
        "      for train_index, test_index in kf.split(x_tr):\n",
        "        print(test_index)\n",
        "        # Split the data into training and test sets for this fold\n",
        "        x_train, x_test = x_tr[train_index], x_tr[test_index]\n",
        "        x_train, x_test=clip_and_z_score( x_train, x_test)\n",
        "        # Get the corresponding features for this layer\n",
        "        trainfeatures, testfeatures = train_features[i][train_index], train_features[i][test_index]\n",
        "        # Compute correlations using Kernel Ridge Regression\n",
        "        correlation_coefficients, y_pred = get_correlations_KERNEL(trainfeatures, testfeatures, x_train , x_test)\n",
        "        # Store predictions\n",
        "        y_heat[test_index] = y_pred\n",
        "        print(f\"the MODEL is: {number_of_model}\")\n",
        "        number_of_model += 1\n",
        "\n",
        "      # Store predictions for the current layer\n",
        "      y_pred_list.append(y_heat)\n",
        "      #Compute correlations\n",
        "      correlation_coefficients_new =pearson_correlation_coefficient(y_heat,x_tr,0)\n",
        "      correlation_coefficients_list.append(correlation_coefficients_new)\n",
        "\n",
        "\n",
        "\n",
        "  correlation_coefficients=np.array(correlation_coefficients_list)\n",
        "  print(correlation_coefficients.shape)\n",
        "  y_pred_list=np.array(y_pred_list)\n",
        "\n",
        "  # Define layers to study in detail\n",
        "  layers_to_study_full = [1,2,3,4, 5]\n",
        "  important_correlations_total.append(correlation_coefficients_list)\n",
        "  print(\"I am added\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tRCvmOL8wXtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_first_part(correlations_list, layers_to_study):\n",
        "    \"\"\"\n",
        "    Filters out significant voxels based on correlation and p-values, and identifies the most significant layer for each voxel.\n",
        "\n",
        "    Parameters:\n",
        "    - correlations_list (list): A list of tuples containing correlation coefficients and p-values for each layer.\n",
        "    - layers_to_study (list): A list of layers to study, where layer indices are 1-based.\n",
        "\n",
        "    Returns:\n",
        "    - layers_result (list): A list indicating the most significant layer index for each voxel.\n",
        "    - significant_voxels (list): A list of voxel indices that have at least one significant correlation.\n",
        "    \"\"\"\n",
        "    # Convert 1-based layer indices to 0-based for indexing\n",
        "    layers_to_study = [layer - 1 for layer in layers_to_study]\n",
        "    dummy_conter=0\n",
        "\n",
        "    layers_result = []\n",
        "    significant_voxels=[]\n",
        "\n",
        "    # Iterate through each voxel\n",
        "    for voxel in range(len(correlations_list[0][0])):\n",
        "        # Extract correlation coefficients and p-values for the current voxel across specified layers\n",
        "        correlation_coefficients_voxel = np.array([correlations_list[layer][0][voxel] for layer in layers_to_study])\n",
        "        p_values_array = np.array([correlations_list[layer][1][voxel] for layer in layers_to_study])\n",
        "        # Check if any p-value is less than or equal to 0.05 (significant)\n",
        "        if np.any(p_values_array <= 0.05):\n",
        "          significant_voxels.append(voxel)\n",
        "          # Identify the layer with the maximum correlation coefficient for this voxel\n",
        "          layer_index = np.argmax(correlation_coefficients_voxel)\n",
        "          layers_result.append(layer_index + 1) #store layers\n",
        "    return layers_result,significant_voxels\n",
        "\n"
      ],
      "metadata": {
        "id": "oT9IsBRbxhr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tot_result=[]\n",
        "# List of layers to study\n",
        "layers_to_study_full = [1,2,3,4, 5]\n",
        "sig_v=[]\n",
        "# Iterate through each brain area in important_correlations_total\n",
        "for area in important_correlations_total:\n",
        "  result_per_area=[]\n",
        "  # Filter significant voxels and get the most significant layer indices for the current area\n",
        "  index_gery,significant_voxels=filter_first_part(area, layers_to_study_full)\n",
        "  # Append the results to the total results list\n",
        "  tot_result.append(index_gery)\n",
        "  sig_v.append(significant_voxels)\n",
        "  # Print the number of significant voxels found for the current area\n",
        "  print(len(index_gery))"
      ],
      "metadata": {
        "id": "7obs9HmPz7wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through each list of significant layer indices in tot_result\n",
        "for i in tot_result:\n",
        "  # Get the unique elements and their counts\n",
        "  unique_elements, counts = np.unique(i, return_counts=True)\n",
        "  # Create a dictionary mapping each unique element to its count\n",
        "  element_counts = dict(zip(unique_elements, counts))\n",
        "  # Print the dictionary of element counts\n",
        "  print(element_counts)"
      ],
      "metadata": {
        "id": "q7dBxqGNauUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to save your results\n",
        "folder_path=\"/content/drive/MyDrive/GOD-NEW-3p/Neural-DecodingPartProgresive/\"\n",
        "with open(f'{folder_path}indeces_filtered_clipped3.dat', 'wb') as fp:\n",
        "            pickle.dump(sig_v, fp)\n"
      ],
      "metadata": {
        "id": "xmJsmbd8d7gq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}